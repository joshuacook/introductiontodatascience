{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a probabilistic perspective we seek the model estimate $\\widehat{f}$ that is most probable given the data. Let $H_f$ be the hypothesis that a specific model is the correct model and $p(D)$ represent the probability of the data, then we seek $\\widehat{f}$ that maximizes, that is we seek \n",
    "\n",
    "$$\\widehat{f} = \\underset{f}{\\mathrm{argmax}}\\left(p(H_f|D)\\right)$$\n",
    "\n",
    "We can use Bayes' Rule to invert this probability so that\n",
    "\n",
    "$$\\widehat{f} = \\underset{f}{\\mathrm{argmax}}\\left(\\frac{p(D|H_f)p(H_f)}{p(D)}\\right)$$\n",
    "\n",
    "If we consider that each model is equally likely and that the probability of the data $p(D)$ is a constant applied to every calculation, then without loss of generality, we can say \n",
    "\n",
    "$$\\widehat{f} = \\underset{f}{\\mathrm{argmax}}\\left(p(D|H_f)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Regression setting, it can be shown that maximizing this equation is equivalent to minimizing the residual sum of squares, \n",
    "\n",
    "$$\\text{RSS} = \\sum (y_i - \\widehat{y})^2 = (\\mathbf{y} -\\mathbf{\\widehat{y}})^T(\\mathbf{y} -\\mathbf{\\widehat{y}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Model Hypotheses\n",
    "\n",
    "For this particular task we will consider the hypothesis space to be all of the linear models possible given our data sets. If we recognize that the set of all of the models possible with `dataset_1` is a subset of all of the models possible with `dataset_2` and likewise with `dataset_3` and `dataset_4`, then we can take the size of this hypothesis space to be all of the possible models made using `dataset_2` and all of the possible models made using `dataset_4`.\n",
    "\n",
    "This is a vast space. \n",
    "\n",
    "To get a sense of the size of this hypothesis space, let us consider a simpler data set, one that has just two features, $x_1$ and $x_2$. We note that there are four possible models using these two features:\n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{f}_1 &= \\beta_0 \\\\\n",
    "\\widehat{f}_2 &= \\beta_0 + \\beta_1x_1\\\\\n",
    "\\widehat{f}_3 &= \\beta_0 + \\beta_1x_2\\\\\n",
    "\\widehat{f}_4 &= \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Similarly for a data set with three features, there will be eight possible models:\n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{f}_1 &= \\beta_0 \\\\\n",
    "\\widehat{f}_2 &= \\beta_0 + \\beta_1x_1\\\\\n",
    "\\widehat{f}_3 &= \\beta_0 + \\beta_1x_2\\\\\n",
    "\\widehat{f}_4 &= \\beta_0 + \\beta_1x_3\\\\\n",
    "\\widehat{f}_5 &= \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\\\\n",
    "\\widehat{f}_6 &= \\beta_0 + \\beta_1x_1 + \\beta_2x_3\\\\\n",
    "\\widehat{f}_7 &= \\beta_0 + \\beta_1x_2 + \\beta_2x_3\\\\\n",
    "\\widehat{f}_8 &= \\beta_0 + \\beta_1x_1 + \\beta_2x_2+ \\beta_3x_3\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially we are forming what is known as a Power Set of possible feature combinations. The number of elements in a Power Set is given by $2^p$ where $p$ is the number of elements in the set. For our current dataset we have 390 features. This is a hypothesis space With a dimension of $2.5\\times10^{117}$. If we trained one model per second, it would take us $8\\times10^{109}$ years to search the entire hypothesis space. For perspective Physicists estimate that the universe is approximately $13.8\\times10^{9}$ years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we are not going to be able to exhaustively search this hypothesis space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data = read.csv(\"data/iris.csv\", row.names='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>sepal_length</th><th scope=col>sepal_width</th><th scope=col>petal_length</th><th scope=col>petal_width</th><th scope=col>label</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & sepal\\_length & sepal\\_width & petal\\_length & petal\\_width & label\\\\\n",
       "\\hline\n",
       "\t0 & 5.1 & 3.5 & 1.4 & 0.2 & 0  \\\\\n",
       "\t1 & 4.9 & 3.0 & 1.4 & 0.2 & 0  \\\\\n",
       "\t2 & 4.7 & 3.2 & 1.3 & 0.2 & 0  \\\\\n",
       "\t3 & 4.6 & 3.1 & 1.5 & 0.2 & 0  \\\\\n",
       "\t4 & 5.0 & 3.6 & 1.4 & 0.2 & 0  \\\\\n",
       "\t5 & 5.4 & 3.9 & 1.7 & 0.4 & 0  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | sepal_length | sepal_width | petal_length | petal_width | label | \n",
       "|---|---|---|---|---|---|\n",
       "| 0 | 5.1 | 3.5 | 1.4 | 0.2 | 0   | \n",
       "| 1 | 4.9 | 3.0 | 1.4 | 0.2 | 0   | \n",
       "| 2 | 4.7 | 3.2 | 1.3 | 0.2 | 0   | \n",
       "| 3 | 4.6 | 3.1 | 1.5 | 0.2 | 0   | \n",
       "| 4 | 5.0 | 3.6 | 1.4 | 0.2 | 0   | \n",
       "| 5 | 5.4 | 3.9 | 1.7 | 0.4 | 0   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  sepal_length sepal_width petal_length petal_width label\n",
       "0 5.1          3.5         1.4          0.2         0    \n",
       "1 4.9          3.0         1.4          0.2         0    \n",
       "2 4.7          3.2         1.3          0.2         0    \n",
       "3 4.6          3.1         1.5          0.2         0    \n",
       "4 5.0          3.6         1.4          0.2         0    \n",
       "5 5.4          3.9         1.7          0.4         0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = \"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\", \n",
       "    data = iris.data)\n",
       "\n",
       "Deviance Residuals: \n",
       "     Min        1Q    Median        3Q       Max  \n",
       "-0.59046  -0.15230   0.01338   0.10332   0.55061  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)   0.19208    0.20470   0.938 0.349611    \n",
       "sepal_length -0.10974    0.05776  -1.900 0.059418 .  \n",
       "sepal_width  -0.04424    0.05996  -0.738 0.461832    \n",
       "petal_length  0.22700    0.05699   3.983 0.000107 ***\n",
       "petal_width   0.60989    0.09447   6.456 1.52e-09 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for gaussian family taken to be 0.04798457)\n",
       "\n",
       "    Null deviance: 100.0000  on 149  degrees of freedom\n",
       "Residual deviance:   6.9578  on 145  degrees of freedom\n",
       "AIC: -22.935\n",
       "\n",
       "Number of Fisher Scoring iterations: 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris.glm = glm(\"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\", data = iris.data)\n",
    "summary(iris.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Log-Likelihood\n",
    "\n",
    "Without going too far into the math, we can think of the log-likelihood as a **likelihood function** telling us how likely a model is given the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is not human interpretable but is useful as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log Lik.' 17.46751 (df=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logLik(iris.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"All models are wrong, but some are useful.\" - George Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be concerned with one additional property - the **complexity** of the model. \n",
    "\n",
    "##### William of Occam\n",
    "\n",
    "[**Occam's razor**](https://en.wikipedia.org/wiki/Occam's_razor) is the problem-solving principle that, when presented with competing hypothetical answers to a problem, one should select the one that makes the fewest assumptions.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ab/William_of_Ockham_-_Logica_1341.jpg\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent this idea of complexity in terms of both the number of features we use and the amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
    "\n",
    "The BIC is formally defined as\n",
    "\n",
    "$$ \\mathrm{BIC} = {\\ln(n)k - 2\\ln({\\widehat L})}. $$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\widehat L$ = the maximized value of the likelihood function of the model $M$\n",
    "- $x$ = the observed data\n",
    "- $n$ = the number of data points in $x$, the number of observations, or equivalently, the sample size;\n",
    "- $k$ = the number of parameters estimated by the model. For example, in multiple linear regression, the estimated parameters are the intercept, the $q$ slope parameters, and the constant variance of the errors; thus, $k = q + 2$.\n",
    "\n",
    "\n",
    "It might help us to think of it as \n",
    "\n",
    "$$ \\mathrm{BIC} = \\text{complexity}-\\text{likelihood}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "-4.87121487462612"
      ],
      "text/latex": [
       "-4.87121487462612"
      ],
      "text/markdown": [
       "-4.87121487462612"
      ],
      "text/plain": [
       "[1] -4.871215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BIC(iris.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log Lik.' -4.871215 (df=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = length(iris.glm$fitted.values)\n",
    "p = length(coefficients(iris.glm))\n",
    "\n",
    "likelihood = 2 * logLik(iris.glm)\n",
    "complexity = log(n)*(p+1)\n",
    "\n",
    "bic = complexity - likelihood\n",
    "bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIC_of_model = function (model) {\n",
    "    n = length(model$fitted.values)\n",
    "    p = length(coefficients(model))\n",
    "\n",
    "    likelihood = 2 * logLik(model)\n",
    "    complexity = log(n)*(p+1)\n",
    "\n",
    "    bic = complexity - likelihood\n",
    "    return(bic)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log Lik.' -4.871215 (df=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BIC_of_model(iris.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Here, we choose the optimal model by removing features one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1  = \"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\"\n",
    "model_2a = \"label ~ 1 + sepal_length + sepal_width + petal_length\"\n",
    "model_2b = \"label ~ 1 + sepal_length + sepal_width                + petal_width\"\n",
    "model_2c = \"label ~ 1 + sepal_length               + petal_length + petal_width\"\n",
    "model_2d = \"label ~ 1                + sepal_width + petal_length + petal_width\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.glm.1 = glm(model_1, data=iris.data)\n",
    "iris.glm.2a = glm(model_2a, data=iris.data)\n",
    "iris.glm.2b = glm(model_2b, data=iris.data)\n",
    "iris.glm.2c = glm(model_2c, data=iris.data)\n",
    "iris.glm.2d = glm(model_2d, data=iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"model_1\"           \"-4.87121487462612\"\n",
      "[1] \"model_2a\"         \"28.0137935908893\"\n",
      "[1] \"model_2b\"         \"5.69337438932066\"\n",
      "[1] \"model_2c\"          \"-9.31979403027607\"\n",
      "[1] \"model_2d\"         \"-6.1930960954627\"\n"
     ]
    }
   ],
   "source": [
    "print(c('model_1', BIC_of_model(iris.glm.1)))\n",
    "print(c('model_2a', BIC_of_model(iris.glm.2a )))\n",
    "print(c('model_2b', BIC_of_model(iris.glm.2b )))\n",
    "print(c('model_2c', BIC_of_model(iris.glm.2c )))\n",
    "print(c('model_2d', BIC_of_model(iris.glm.2d )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"model_1\"           \"-4.87121487462612\"\n",
      "[1] \"model_2a\"         \"28.0137935908893\"\n",
      "[1] \"model_2b\"         \"5.69337438932066\"\n",
      "[1] \"model_2c\"          \"-9.31979403027607\"\n",
      "[1] \"model_2d\"         \"-6.1930960954627\"\n"
     ]
    }
   ],
   "source": [
    "print(c('model_1', BIC(iris.glm.1)))\n",
    "print(c('model_2a', BIC(iris.glm.2a )))\n",
    "print(c('model_2b', BIC(iris.glm.2b )))\n",
    "print(c('model_2c', BIC(iris.glm.2c )))\n",
    "print(c('model_2d', BIC(iris.glm.2d )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1  = \"label ~ 1 + sepal_length + sepal_width + petal_length + petal_width\"\n",
    "model_2c = \"label ~ 1 + sepal_length               + petal_length + petal_width\"\n",
    "model_3a = \"label ~ 1 + sepal_length               + petal_length \"\n",
    "model_3b = \"label ~ 1 + sepal_length                              + petal_width\"\n",
    "model_3c = \"label ~ 1                              + petal_length + petal_width\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.glm.3a = glm(model_3a, data=iris.data)\n",
    "iris.glm.3b = glm(model_3b, data=iris.data)\n",
    "iris.glm.3c = glm(model_3c, data=iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"model_1\"           \"-4.87121487462612\"\n",
      "[1] \"model_2c\"          \"-9.31979403027607\"\n",
      "[1] \"model_3a\"         \"25.3174210943167\"\n",
      "[1] \"model_3b\"         \"15.4504250116728\"\n",
      "[1] \"model_3c\"         \"-5.0467304546584\"\n"
     ]
    }
   ],
   "source": [
    "print(c('model_1', BIC(iris.glm.1)))\n",
    "print(c('model_2c', BIC(iris.glm.2c )))\n",
    "print(c('model_3a', BIC(iris.glm.3a )))\n",
    "print(c('model_3b', BIC(iris.glm.3b )))\n",
    "print(c('model_3c', BIC(iris.glm.3c )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
