{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "run src/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next phase of this project we move into developing our machine learning models. We have previously about model selection and have considered managing the Bias-Variance Tradeoff as we fit our predictive model. We primarily focused on identifying the simplest possible model as a way to making sure that our model generalizes to new data. Now we expand on this by examining three new concepts in model assessment and selection.\n",
    "\n",
    "1. using cross-validation to study model variance\n",
    "1. applying regularization to help our models generalize\n",
    "1. using emsembling to help our models generalize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly held misconceptions is that cross-validation can to help models to generalize. This is not the case. Rather, cross-validation can be used to help to identify potential issues and to optimize model hyperparameters toward the end of choosing the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling technique and is simply the creative use of collected data. We have already seen a very simple cross-validation approach, the train-test split also called The Validation Set Approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](doc/img/Chapter5/5-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1444, 382), (1444, 390), (1444, 382), (1444, 390))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset_1.shape,\n",
    " dataset_2.shape,\n",
    " dataset_3.shape,\n",
    " dataset_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(dataset_1.index, target_1.index)\n",
    "np.testing.assert_allclose(dataset_2.index, target_2.index)\n",
    "np.testing.assert_allclose(dataset_3.index, target_3.index)\n",
    "np.testing.assert_allclose(dataset_4.index, target_4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttsplit_1 = train_test_split(dataset_1, target_1, test_size=0.4, random_state=0)\n",
    "ttsplit_2 = train_test_split(dataset_2, target_2, test_size=0.4, random_state=0)\n",
    "ttsplit_3 = train_test_split(dataset_3, target_3, test_size=0.4, random_state=0)\n",
    "ttsplit_4 = train_test_split(dataset_4, target_4, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score(model, data):\n",
    "    X_train = data[0]\n",
    "    X_test  = data[1]\n",
    "    y_train = data[2]\n",
    "    y_test  = data[3]\n",
    "    \n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time() - start \n",
    "    return model.score(X_test, y_test), end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.89860862751808601, 0.061138153076171875)\n",
      "(0.89858121850018868, 0.0879824161529541)\n",
      "(0.89924977700919984, 0.02467799186706543)\n",
      "(0.8993019594303594, 0.016490697860717773)\n"
     ]
    }
   ],
   "source": [
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_1))\n",
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_2))\n",
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_3))\n",
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.87587594870369756, 1.8139793872833252)\n",
      "(0.87587068760144149, 13.942325830459595)\n",
      "(0.87344492815283581, 4.486565113067627)\n",
      "(0.8734547160862195, 9.617893695831299)\n"
     ]
    }
   ],
   "source": [
    "print(fit_score(Lasso(max_iter=1E4), ttsplit_1))\n",
    "print(fit_score(Lasso(max_iter=1E5), ttsplit_2))\n",
    "print(fit_score(Lasso(max_iter=1E4), ttsplit_3))\n",
    "print(fit_score(Lasso(max_iter=1E5), ttsplit_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-One-Out Cross-Validation\n",
    "\n",
    "An alternative to using a single validation set is using **leave-one-out cross-validation** (LOOCV). \n",
    "\n",
    "![](doc/img/Chapter5/5-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, instead of creating two sets, we create $n$ sets and fit $n$ models. Using this method, each data point is used as a testing point exactly once. To assess the performance we simply take the average over all models\n",
    "\n",
    "$$\\text{CV}_n=\\mathbb{E}\\left[MSE(f_i)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One draw back to this approach is the substantial time required to set a model for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score_loo(model, dataset, target):\n",
    "    loo = LeaveOneOut()\n",
    "    scores = []\n",
    "    start = time()\n",
    "    for train, test in loo.split(dataset, target):\n",
    "        train = dataset.index[train]\n",
    "        test = dataset.index[test]\n",
    "\n",
    "        X_train = dataset.loc[train]\n",
    "        X_test  = dataset.loc[test]\n",
    "        y_train = dataset.loc[train]\n",
    "        y_test  = dataset.loc[test]\n",
    "    \n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "\n",
    "    end = time() - start \n",
    "    scores = np.array(scores)\n",
    "    print(\"Mean: {:6} Variance: {:6} Time: {:6}\".format(scores.mean(), scores.var(), end))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:    0.0 Variance:    0.0 Time: 179.01318764686584\n",
      "None\n",
      "Mean:    0.0 Variance:    0.0 Time: 187.70759654045105\n",
      "None\n",
      "Mean:    0.0 Variance:    0.0 Time: 167.30623126029968\n",
      "None\n",
      "Mean:    0.0 Variance:    0.0 Time: 159.89035868644714\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fit_score_loo(Ridge(), dataset_1, target_1))\n",
    "print(fit_score_loo(Ridge(), dataset_2, target_2))\n",
    "print(fit_score_loo(Ridge(), dataset_3, target_3))\n",
    "print(fit_score_loo(Ridge(), dataset_4, target_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:    0.0 Variance:    0.0 Time: 5569.9769694805145\n",
      "None\n",
      "Mean:    0.0 Variance:    0.0 Time: 5727.205169916153\n",
      "None\n",
      "Mean:    0.0 Variance:    0.0 Time: 5659.567879199982\n",
      "None\n",
      "Mean:    0.0 Variance:    0.0 Time: 5761.360241889954\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fit_score_loo(Lasso(), dataset_1, target_1))\n",
    "print(fit_score_loo(Lasso(), dataset_2, target_2))\n",
    "print(fit_score_loo(Lasso(), dataset_3, target_3))\n",
    "print(fit_score_loo(Lasso(), dataset_4, target_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually not practical to use LOOCV. Unacceptable alternative is to use **k-fold cross-validation** (KCV). In this method the data set is split into $k$ groups. Then, $k$ models are fit. Uses exactly one of the groups as a validation set And the remaining data as the training set. As before, the cross validation score is simply the average of the scores across all of the models\n",
    "\n",
    "$$\\text{CV}_k=\\mathbb{E}\\left[MSE(f_i)\\right]$$\n",
    "\n",
    "![](doc/img/Chapter5/5-5.png)\n",
    "\n",
    "Typical values of $k$ are $k=5$ or $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame([\n",
    "    {'x' : 1},\n",
    "    {'x' : 2},\n",
    "    {'x' : 3},\n",
    "    {'x' : 4},\n",
    "    {'x' : 5},\n",
    "    {'x' : 6},\n",
    "    {'x' : 7},\n",
    "    {'x' : 8},\n",
    "    {'x' : 9},\n",
    "    {'x' : 10},\n",
    "])\n",
    "\n",
    "X_df.index = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X_df.drop('E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x\n",
       "A   1\n",
       "B   2\n",
       "C   3\n",
       "D   4\n",
       "F   6\n",
       "G   7\n",
       "H   8\n",
       "I   9\n",
       "J  10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "splitter = kf.split(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = next(splitter)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_df.index[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['D', 'F', 'G', 'H', 'I', 'J'], dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score_kfold(model, dataset, target, folds=5):\n",
    "    kf = KFold(n_splits=folds)\n",
    "    scores = []\n",
    "    start = time()\n",
    "    for train, test in kf.split(dataset, target):\n",
    "        train = dataset.index[train]\n",
    "        test = dataset.index[test]\n",
    "\n",
    "        X_train = dataset.loc[train]\n",
    "        X_test  = dataset.loc[test]\n",
    "        y_train = target.loc[train]\n",
    "        y_test  = target.loc[test]\n",
    "    \n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    end = time() - start \n",
    "\n",
    "    print(\"Mean: {:6} Variance: {:6} Time: {:6}\".format(scores.mean(), scores.var(), end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.8853121172449191 Variance: 0.0002229144246784079 Time: 0.1946556568145752\n",
      "Mean: 0.8853098599136107 Variance: 0.00022254648260988997 Time: 0.24270319938659668\n",
      "Mean: 0.8855857104448723 Variance: 0.00022384015204386532 Time: 0.2041628360748291\n",
      "Mean: 0.8855848356167636 Variance: 0.00022431601124553446 Time: 0.33058595657348633\n"
     ]
    }
   ],
   "source": [
    "fit_score_kfold(Ridge(), dataset_1, target_1)\n",
    "fit_score_kfold(Ridge(), dataset_2, target_2)\n",
    "fit_score_kfold(Ridge(), dataset_3, target_3)\n",
    "fit_score_kfold(Ridge(), dataset_4, target_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.870535279080752 Variance: 0.000367807488798026 Time: 3.989431381225586\n",
      "Mean: 0.8705024797773 Variance: 0.00037061494130101127 Time: 3.5346145629882812\n",
      "Mean: 0.8676419438563763 Variance: 0.00021040580637601814 Time: 3.3818252086639404\n",
      "Mean: 0.8675954486257649 Variance: 0.00020691094833489974 Time: 3.176971912384033\n"
     ]
    }
   ],
   "source": [
    "fit_score_kfold(Lasso(), dataset_1, target_1)\n",
    "fit_score_kfold(Lasso(), dataset_2, target_2)\n",
    "fit_score_kfold(Lasso(), dataset_3, target_3)\n",
    "fit_score_kfold(Lasso(), dataset_4, target_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Shuffle Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearnearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-Off for k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of bias, it is clear that LOOCV will have lower bias than KCV when $k < n$. This is because each model is trained using $n-1$ points which is nearly all of the training data. Since KCV uses less of the data, it has less ability to learn the phenomenon represented by the data and is therefore more biased then LOOCV.\n",
    "\n",
    "On the other hand, LOOCV has more variance than KCV. This is because LOOCV involve the fitting and then averaging of performance of $n$ models, whereas KCV does this over $k$ models. Furthermore, the $n$ LOOCV models are more correlated with each other than are the $k$ KCV models. This is clear because each LOOCV model is identical to any other LOOCV model save for one point. Meanwhile each KCV model differs from any other KCV model in $n/k$ points. It can be shown that the meani of highly correlated quantities has higher variance then does the mean of quantities that are not as highly correlated. In other words, the LOOCV has higher variance than does the KCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
