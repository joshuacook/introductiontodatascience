{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run src/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next phase of this project we move into developing our machine learning models. We have previously about model selection and have considered managing the Bias-Variance Tradeoff as we fit our predictive model. We primarily focused on identifying the simplest possible model as a way to making sure that our model generalizes to new data. Now we expand on this by examining three new concepts in model assessment and selection.\n",
    "\n",
    "1. using cross-validation to study model variance\n",
    "1. applying regularization to help our models generalize\n",
    "1. using emsembling to help our models generalize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly held misconceptions is that cross-validation can to help models to generalize. This is not the case. Rather, cross-validation can be used to help to identify potential issues and to optimize model hyperparameters toward the end of choosing the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling technique and is simply the creative use of collected data. We have already seen a very simple cross-validation approach, the train-test split also called The Validation Set Approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](doc/img/Chapter5/5-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1444, 382), (1444, 390), (1444, 382), (1444, 390))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset_1.shape,\n",
    " dataset_2.shape,\n",
    " dataset_3.shape,\n",
    " dataset_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(dataset_1.index, target_1.index)\n",
    "np.testing.assert_allclose(dataset_2.index, target_2.index)\n",
    "np.testing.assert_allclose(dataset_3.index, target_3.index)\n",
    "np.testing.assert_allclose(dataset_4.index, target_4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttsplit_1 = train_test_split(dataset_1, target_1, test_size=0.4, random_state=0)\n",
    "ttsplit_2 = train_test_split(dataset_2, target_1, test_size=0.4, random_state=0)\n",
    "ttsplit_3 = train_test_split(dataset_3, target_1, test_size=0.4, random_state=0)\n",
    "ttsplit_4 = train_test_split(dataset_4, target_1, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score(model, data):\n",
    "    X_train = data[0]\n",
    "    X_test  = data[1]\n",
    "    y_train = data[2]\n",
    "    y_test  = data[3]\n",
    "    \n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time() - start \n",
    "    return model.score(X_test, y_test), end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.89860862751808546, 0.05988335609436035)\n",
      "(0.89858127471660521, 0.04999995231628418)\n",
      "(0.89924977700919972, 0.11284351348876953)\n",
      "(0.89930376234587361, 0.15570759773254395)\n"
     ]
    }
   ],
   "source": [
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_1))\n",
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_2))\n",
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_3))\n",
    "print(fit_score(Ridge(max_iter=1E5), ttsplit_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.87587594870369756, 2.7831077575683594)\n",
      "(0.87587066389618007, 14.507377624511719)\n",
      "(0.87344492815283581, 1.4013075828552246)\n",
      "(0.87345792431143909, 8.512555122375488)\n"
     ]
    }
   ],
   "source": [
    "print(fit_score(Lasso(max_iter=1E4), ttsplit_1))\n",
    "print(fit_score(Lasso(max_iter=1E5), ttsplit_2))\n",
    "print(fit_score(Lasso(max_iter=1E4), ttsplit_3))\n",
    "print(fit_score(Lasso(max_iter=1E5), ttsplit_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-One-Out Cross-Validation\n",
    "\n",
    "An alternative to using a single validation set is using **leave-one-out cross-validation** (LOOCV). \n",
    "\n",
    "![](doc/img/Chapter5/5-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, instead of creating two sets, we create $n$ sets and fit $n$ models. Using this method, each data point is used as a testing point exactly once. To assess the performance we simply take the average over all models\n",
    "\n",
    "$$\\text{CV}_n=\\mathbb{E}\\left[MSE(f_i)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One draw back to this approach is the substantial time required to set a model for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score_loo(model, dataset, target):\n",
    "    loo = LeaveOneOut()\n",
    "    scores = []\n",
    "    for train, test in loo.split(dataset, target):\n",
    "        train = dataset.index[train]\n",
    "        test = dataset.index[test]\n",
    "\n",
    "        X_train = dataset.loc[train]\n",
    "        X_test  = dataset.loc[test]\n",
    "        y_train = dataset.loc[train]\n",
    "        y_test  = dataset.loc[test]\n",
    "    \n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    print(\"Mean: {} Variance: {}\").format(scores.mean(), scores.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fit_score_loo(Ridge(), dataset_1, target_1))\n",
    "# print(fit_score_loo(Ridge(), dataset_2, target_2))\n",
    "# print(fit_score_loo(Ridge(), dataset_3, target_3))\n",
    "# print(fit_score_loo(Ridge(), dataset_4, target_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fit_score_loo(Lasso(), dataset_1, target_1))\n",
    "# print(fit_score_loo(Lasso(), dataset_2, target_2))\n",
    "# print(fit_score_loo(Lasso(), dataset_3, target_3))\n",
    "# print(fit_score_loo(Lasso(), dataset_4, target_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually not practical to use LOOCV. Unacceptable alternative is to use **k-fold cross-validation** (KCV). In this method the data set is split into $k$ groups. Then, $k$ models are fit. Uses exactly one of the groups as a validation set And the remaining data as the training set. As before, the cross validation score is simply the average of the scores across all of the models\n",
    "\n",
    "$$\\text{CV}_k=\\mathbb{E}\\left[MSE(f_i)\\right]$$\n",
    "\n",
    "![](doc/img/Chapter5/5-5.png)\n",
    "\n",
    "Typical values of $k$ are $k=5$ or $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score_kfold(model, dataset, target, folds=5):\n",
    "    kf = KFold(n_splits=folds)\n",
    "    scores = []\n",
    "    start = time()\n",
    "    for train, test in kf.split(dataset, target):\n",
    "        train = dataset.index[train]\n",
    "        test = dataset.index[test]\n",
    "\n",
    "        X_train = dataset.loc[train]\n",
    "        X_test  = dataset.loc[test]\n",
    "        y_train = dataset.loc[train]\n",
    "        y_test  = dataset.loc[test]\n",
    "    \n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    end = time() - start \n",
    "\n",
    "    print(\"Mean: {:6} Variance: {:6} Time: {:6}\".format(scores.mean(), scores.var(), end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.9981262140537511 Variance: 9.051588022447506e-08 Time: 8.507905960083008\n",
      "Mean: 0.99856147117169 Variance: 5.8902910436906337e-08 Time: 6.726489067077637\n",
      "Mean: 0.9969821835724456 Variance: 2.5471613800450136e-07 Time: 5.750875234603882\n",
      "Mean: 0.9976435153720546 Variance: 1.59744732253966e-07 Time: 6.65703558921814\n"
     ]
    }
   ],
   "source": [
    "fit_score_kfold(Ridge(), dataset_1, target_1)\n",
    "fit_score_kfold(Ridge(), dataset_2, target_2)\n",
    "fit_score_kfold(Ridge(), dataset_3, target_3)\n",
    "fit_score_kfold(Ridge(), dataset_4, target_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.0005571268766496533 Variance: 2.7351592891329313e-06 Time: 39.25697994232178\n",
      "Mean: 0.15485748423960147 Variance: 5.6508959079436564e-05 Time: 40.771477460861206\n",
      "Mean: -0.003698071537686329 Variance: 2.3624052947981027e-07 Time: 39.45966958999634\n",
      "Mean: 0.0849277256112875 Variance: 1.9073768861526476e-06 Time: 40.17126703262329\n"
     ]
    }
   ],
   "source": [
    "fit_score_kfold(Lasso(), dataset_1, target_1)\n",
    "fit_score_kfold(Lasso(), dataset_2, target_2)\n",
    "fit_score_kfold(Lasso(), dataset_3, target_3)\n",
    "fit_score_kfold(Lasso(), dataset_4, target_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-Off for k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of bias, it is clear that LOOCV will have lower bias than KCV when $k < n$. This is because each model is trained using $n-1$ points which is nearly all of the training data. Since KCV uses less of the data, it has less ability to learn the phenomenon represented by the data and is therefore more biased then LOOCV.\n",
    "\n",
    "On the other hand, LOOCV has more variance than KCV. This is because LOOCV involve the fitting and then averaging of performance of $n$ models, whereas KCV does this over $k$ models. Furthermore, the $n$ LOOCV models are more correlated with each other than are the $k$ KCV models. This is clear because each LOOCV model is identical to any other LOOCV model save for one point. Meanwhile each KCV model differs from any other KCV model in $n/k$ points. It can be shown that the meani of highly correlated quantities has higher variance then does the mean of quantities that are not as highly correlated. In other words, the LOOCV has higher variance than does the KCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
