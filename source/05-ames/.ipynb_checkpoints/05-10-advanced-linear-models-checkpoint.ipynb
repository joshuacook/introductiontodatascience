{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, linear models are fit by minimizing a loss function. This function will be a function of the $\\beta$ coefficients for each feature. The best set of $\\beta$ values is the set that gives the smallest value of the loss function. \n",
    "\n",
    "The most common loss function is \n",
    "\n",
    "$$\\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left(y - \\widehat{y}\\right)^2 = (y-X\\beta)^T(y-X\\beta)$$\n",
    "\n",
    "Generally speaking, this equation is a second-degree polynomial or quadratic. \n",
    "\n",
    "If we had one feature, then we would be seeking the best line\n",
    "\n",
    "$$\\widehat{f} = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "The loss $\\mathcal{L}$ as a function of $\\beta_1$ would be the familiar parabola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbeta = np.linspace(-3,3,1000)\n",
    "lloss = xx**2\n",
    "plt.plot(bbeta, lloss)\n",
    "plt.xlabel('$\\\\beta_1$')\n",
    "plt.ylabel('$\\\\mathcal{L}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to plot the loss versus $\\beta_0$ and $\\beta_1$, it would be a 3-D paraboloid where the height of the paraboloid is the associated loss $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Paraboloid_of_Revolution.svg/512px-Paraboloid_of_Revolution.svg.png\" width=200px>\n",
    "\n",
    "This would generalize to higher dimensions. The important thing is that there is always a single **global** minimum i.e. *the loss function is convex*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a Minimum\n",
    "\n",
    "Given a parabol(oid), we can find the minimum using calculus. Recall that the derivative of a function represents the rate of change (slope) of the function at a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbeta = np.linspace(-3,3,1000)\n",
    "lloss = xx**2\n",
    "\n",
    "test_points = np.linspace(-3,3,7)\n",
    "slopes = 2*test_points\n",
    "losses = test_points**2\n",
    "\n",
    "slope_line = lambda m, x, x1, y1: m*(x - x1) + y1\n",
    "plt.plot(bbeta, lloss)\n",
    "for m, x1, y1 in zip(slopes, test_points, losses):\n",
    "    plt.plot(bbeta, slope_line(m, bbeta, x1, y1), label='m = {}'.format(m))\n",
    "plt.ylim(-1, 10)\n",
    "plt.legend()\n",
    "plt.xlabel('$\\\\beta_1$')\n",
    "plt.ylabel('$\\\\mathcal{L}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Calculus, we can find the minimum of our loss function by finding where the slope is zero. In other words, we can find the minimum of the loss function by finding where the derivative of the loss function is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Logistic Regression is a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=2)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "plt.scatter(X[0], X[1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = lr.coef_[0][0]\n",
    "B = lr.coef_[0][1]\n",
    "C = lr.intercept_\n",
    "\n",
    "xx = np.linspace(min(X[0]), max(X[0]))\n",
    "y_hat = lambda x: -A/B*x - C/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[0], X[1], c=y)\n",
    "plt.plot(xx, y_hat(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal with logistic regression is to find the line that \"best splits\" the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that one way to do this would be to have an equation of the form \n",
    "\n",
    "$$\\widehat{y} = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "for one variable. If the equation returns a result greater then 0.5, we would call it class \"1\". It less than 0.5, class \"2\".\n",
    "\n",
    "\n",
    "$$ \\text{Class}(x) = \\left\\{\n",
    "     \\begin{array}{lr}\n",
    "       1 & : \\widehat{y} > 0.5\\\\\n",
    "       0 & : \\widehat{y} \\leq 0.5\n",
    "     \\end{array}\n",
    "   \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-3,3, 1000)\n",
    "y_hat = lambda x: 1 + 0.5*x\n",
    "cls = lambda x: np.piecewise(x, [y_hat(x) > 0.5, y_hat(x) <= 0.5], [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xx, cls(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue with this function in terms of our loss function. The most common loss function is \n",
    "\n",
    "$$\\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left(y - \\widehat{y}\\right)^2 = (y-X\\beta)^T(y-X\\beta)$$\n",
    "\n",
    "Here the possible value of $y$ are the possible classes, that is $y \\in \\{0,1\\}$.\n",
    "\n",
    "Here the possible value of $\\widehat{y}$ are also the possible classes, that is $\\widehat{y} \\in \\{0,1\\}$.\n",
    "\n",
    "The implications of this are that loss function is not continous and more importantly the derivative has points at which it is not defined. **We can not use calculus to solve this problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This led to two innovations that are fundamental to modern machine learning (and especially deep learning):\n",
    "\n",
    "1. the sigmoid\n",
    "1. gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is a way to make the logistic regression equation continuous. Instead of\n",
    "\n",
    "$$ \\widehat{f}(x) = \\left\\{\n",
    "     \\begin{array}{lr}\n",
    "       1 & : \\beta_0 + \\beta_1x > 0.5\\\\\n",
    "       0 & : \\beta_0 + \\beta_1x \\leq 0.5\n",
    "     \\end{array}\n",
    "   \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the sigmoid function \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "to build our logistic regression function so that\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\widehat{f}(x) = \\sigma(\\beta_0 + \\beta_1x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function gets its name because it resembles an \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "xx = np.linspace(-5, 5)\n",
    "yy = 1/(1+np.exp(-xx))\n",
    "plt.plot(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ is very large then $e^{-x}$ is very small and goes to zero, therefore, $\\widehat{f}(x)$ is close to 1.\n",
    "\n",
    "If $x$ is very small then $e^{-x}$ is very large and goes to infinity. Because $\\frac{1}{\\infty}\\sim 0$, we can say that $\\widehat{f}(x)$ is close to 0.\n",
    "\n",
    "Note that if $x$ is zero, then $e^{-x}$ is 1 and therefore $\\widehat{f}(x) = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret the output of the logistic function to be a probability that an input is a certain class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional point. We can think of this in terms of the following DAG:\n",
    "\n",
    "<img src=\"https://www.evernote.com/l/AAE3uJCExaROCKiw2MGh6qZb6C6I8j397bsB/image.png\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the logistic function is now continuous its non-linearity still makes it challenging to work with and in general we do not solve this by setting the derivative of the loss function to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind gradient descent is extremely complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm begins by selecting $\\beta$ values at random. The Data is passed through the pipeline to obtain the loss for the initial $\\beta$ values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.evernote.com/l/AAF5BGvPeBNMBox9FJiupRrV0ptrd9qlpxQB/image.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the loss value, we are able to discern the direction in which we would need to update the $\\beta$ values in order to improve the loss. The direction we need to travel in order to improve the loss is called the **gradient**. \n",
    "\n",
    "Recall that the shape of the loss is a paraboloid. We want to **descend** to the bottom of the loss paraboloid.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Paraboloid_of_Revolution.svg/512px-Paraboloid_of_Revolution.svg.png\" width=200px>\n",
    "\n",
    "To do this we repeatedly pass the known data through the pipeline descending one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalizes to a more complex linear DAG via a process called error or **backward propagation**.\n",
    "\n",
    "<img src=\"https://www.evernote.com/l/AAEbw-A7neVFMrX0jUwWViy3ugHFKKND9lkB/image.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward propagation algorithm is the fundamental algorithm for the branch of machine learning known as **deep learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
