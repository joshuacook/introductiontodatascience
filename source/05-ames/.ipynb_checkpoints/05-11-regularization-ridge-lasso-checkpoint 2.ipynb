{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Overview of regularization\n",
    "\n",
    "---\n",
    "\n",
    "The goal of \"regularizing\" regression models is to **structurally prevent overfitting by imposing a penalty on the coefficients** of the model.\n",
    "\n",
    "Regularization methods like the Ridge and Lasso add this additional \"penalty\" on **the size of coefficients** to the loss function. When the loss function is minimized, this additional component is added to the residual sum of squares.\n",
    "\n",
    "In other words, the minimization becomes a balance between the error between:\n",
    "1. predictions and true values\n",
    "2. the size of the coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most common types of regularization are the **Lasso**, **Ridge**. There is a mixture of them called the **Elastic Net**. We will take a look at the mathematics of regularization and the effect these penalties have on model fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.evernote.com/l/AAH_btO8DnBF8I9__sqWwamIflWoyht43OoB/image.png\"\n",
    "     width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From ISLR, Ch. 6\n",
    "\n",
    "The image on the left represents regularization via the Lasso. The image on the right regularization via a Ridge Regression.\n",
    "\n",
    "The red lines represent countours of a paraboloid signifying the loss function. \n",
    "\n",
    "The point in the center is the nadir of the paraboloid and represents the minimum of the loss function.\n",
    "\n",
    "The trick with regularization is to get as close to the bottom of the paraboloid while staying within the area defined by the regularization function. One interpretation of this is think of the regularizataion as a \"budget\". We want to get to the center of the paraboloid, but we have to stay within our budget to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lsq-loss'></a>\n",
    "\n",
    "## Review: least squares loss function\n",
    "\n",
    "---\n",
    "\n",
    "Ordinary least squares regression minimizes the residual sum of squares (RSS) to fit the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\mathcal{L}(\\beta) = \\sum_{i=1}^n \\epsilon_i^2 =  (y-X\\beta)^T(y-X\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ridge'></a>\n",
    "\n",
    "## The Ridge penalty\n",
    "\n",
    "---\n",
    "\n",
    "Ridge regression adds the sum of the squared (non-intercept!) $\\beta$ values to the loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\mathcal{L}_{ridge}(\\beta) = \\sum_{i=1}^n \\epsilon_i^2 +\\lambda\\sum\\beta_i^2 =  (y-X\\beta)^T(y-X\\beta)+\\lambda\\beta^T\\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to fit the $\\beta$ vector we will minimize $\\mathcal{L}_{ridge}(\\beta)$. \n",
    "\n",
    "Again, we take the derivative so that \n",
    "\n",
    "### $$ \\frac{d }{d \\beta} \\mathcal{L}(\\beta)= \n",
    "2X^TX\\beta\n",
    "-2X^Ty +2\\lambda\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the result equal to zero so that \n",
    "\n",
    "### $$ X^TX\\beta+\\lambda\\beta=X^Ty $$\n",
    "\n",
    "### $$ (X^TX+\\lambda I)\\beta=X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "\n",
    "### $$\\beta = (X^TX+\\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "or\n",
    "\n",
    "    beta = inv(X.T.dot(X)+lambda*np.eye(n)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- $\\beta^T\\beta$ can be written as $||\\beta||_2^2$ and is known as the $\\ell_2$ norm.\n",
    "- $\\lambda$ is a constant for the _strength_ of the regularization parameter. \n",
    "- The higher this value, the greater the impact of this new component in the loss function.\n",
    "- Thinking of our budget for reaching the bottom of the bowl, the **bigger** the $\\lambda$, the **smaller** our budget!\n",
    "- The **bigger** the $\\lambda$, the **smaller** our coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso'></a>\n",
    "\n",
    "## The Lasso penalty\n",
    "\n",
    "---\n",
    "\n",
    "The Lasso regression takes a different approach. Instead of adding the sum of _squared_ $\\beta$ coefficients to the RSS, it adds the sum of the _absolute values_ of the $\\beta$ coefficients:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\mathcal{L}_{lasso}(\\beta) = \\sum_{i=1}^n \\rvert\\epsilon_i\\rvert +\\lambda\\sum\\rvert\\beta_i\\rvert =  (y-X\\beta)^T(y-X\\beta)+\\lambda\\rvert\\beta\\rvert $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** $\\rvert\\beta\\rvert$ is not differentiable.\n",
    "\n",
    "Therefore, there is no closed form solution to the Lasso i.e. we can not solve it using Algebra!\n",
    "\n",
    "Think about an absolute value curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a kink in this function at $x=0$ and the derivative is not continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,500)\n",
    "plt.plot(x,  np.piecewise(x, [x < 0, x >= 0], [lambda x: -x, lambda x: x]), label='abs values function')\n",
    "plt.plot(x,  np.piecewise(x, [x < 0, x >= 0], [-1, 1]), label='derivative')\n",
    "plt.axvline(c='black',lw=.5)\n",
    "plt.axhline(c='black',lw=.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- $\\rvert\\beta\\rvert$ can be written as $|\\beta|_1$ and is known as the $\\ell_1$ norm.\n",
    "- $\\lambda$ is a constant for the _strength_ of the regularization parameter. \n",
    "- The higher this value, the greater the impact of this new component in the loss function.\n",
    "- Thinking of our budget for reaching the bottom of the bowl, the **bigger** the $\\lambda$, the **smaller** our budget!\n",
    "- The **bigger** the $\\lambda$, the **smaller** our coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elastic-net'></a>\n",
    "\n",
    "## Elastic Net penalty\n",
    "\n",
    "---\n",
    "\n",
    "Elastic Net is simply a combination of the Lasso and the Ridge regularizations. It adds *both* penalties to the loss function:\n",
    "\n",
    "### $$\\mathcal{L}_{lasso}(\\beta) = \\sum_{i=1}^n \\rvert\\epsilon_i\\rvert + \\lambda\\sum\\rvert\\beta_i\\rvert+(1-\\lambda)\\sum\\beta_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the elastic net, the effect of the Ridge vs. the Lasso is balanced by the two lambda parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the effect of regularization?\n",
    "\n",
    "---\n",
    "\n",
    "An important aspect of this data, which is a reason why we might choose to use regularization, is that there is **multicollinearity** in the data. The term multicollinearity means that there are high correlations between predictor variables in your model. \n",
    "\n",
    "**This can lead to a variety of problems including:**\n",
    "1. The effect of predictor variables estimated by your regression will depend on what other variabes are included in your model.\n",
    "2. Predictors can have wildly different effects depending on the observations in your sample, and small changes in samples can result in very different estimated effects.\n",
    "3. With very high multicollinearity, the inverse matrix the computer calculates may not be accurate.\n",
    "4. We can no longer interpret a coefficient on a variable as the effect on the target of a one unit increase in that variable holding the other variables constant. This is because when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n",
    "\n",
    "The Ridge is best suited to deal with multicollinearity. Lasso also deals with multicollinearity between variables, but in a more brutal way (it \"zeroes out\" the less effective variable).\n",
    "\n",
    "The Lasso is particularly useful when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you by forcing coefficients to be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the wine csv\n",
    "\n",
    "This version has red and white wines concatenated together and tagged with a binary 1,0 indicator (1 is red wine). There are many other variables purportedly related to the rated quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run src/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2.shape, dataset_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-ridge'></a>\n",
    "\n",
    "## Visualizing the Ridge\n",
    "\n",
    "---\n",
    "\n",
    "Import the `Ridge` model class from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates over a series of different alpha regularization parameters. The alpha is sklearn's equivalent of the lambda value in the formula that multiples the square of betas from the equation.\n",
    "\n",
    "The function stores the results of the model so that we can plot them interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_coefs(X, Y, alphas):\n",
    "    \n",
    "    # set up the list to hold the different sets of coefficients:\n",
    "    coefs = []\n",
    "    \n",
    "    # Set up a ridge regression object\n",
    "    ridge_reg = Ridge()\n",
    "    \n",
    "    # Iterate through the alphas fed into the function:\n",
    "    for a in alphas:\n",
    "        \n",
    "        # On each alpha reset the ridge model's alpha to the current one:\n",
    "        ridge_reg.set_params(alpha=a)\n",
    "        \n",
    "        # fit or refit the model on the provided X, Y\n",
    "        ridge_reg.fit(X, Y)\n",
    "        \n",
    "        # Get out the coefficient list\n",
    "        coefs.append(ridge_reg.coef_)\n",
    "        \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha values for the ridge are best visualized on a logarithmic \"magnitude\" scale. Essentially, the effect of alpha on the coefficients does not increase linearly but by orders of magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.logspace gives us points between specified orders of magnitude on a logarithmic scale. It is base 10.\n",
    "r_alphas = np.logspace(-3, 12, 45)\n",
    "\n",
    "# Get the coefficients for each alpha for the Ridge, using the function above\n",
    "r_coefs = ridge_coefs(dataset_2, target_2, r_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting function below will:\n",
    "\n",
    " - Plot the effect of changing alpha on the coefficient size on a **path** graph\n",
    " - Plot the effect of changing alpha on the coefficient size on a **bar** graph\n",
    " \n",
    "Each one gives informative information. It's just two different ways of visualizing the same thing. The chart is interactive so you can play around with the values of alpha across the specified range above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cycler package lets us \"cycle\" throug colors.\n",
    "# Just another thing i had to look up on stackoverflow. That's my life.\n",
    "from cycler import cycler\n",
    "\n",
    "def coef_plotter(alphas, coefs, to_alpha, regtype='ridge'):\n",
    "    \n",
    "    # Get the full range of alphas before subsetting to keep the plots from \n",
    "    # resetting axes each time. (We use these values to set static axes later).\n",
    "    amin = np.min(alphas)\n",
    "    amax = np.max(alphas)\n",
    "    \n",
    "    # Subset the alphas and coefficients to just the ones below the set limit\n",
    "    # from the interactive widget:\n",
    "    alphas = [a for a in alphas if a <= to_alpha]\n",
    "    coefs = coefs[0:len(alphas)]\n",
    "    \n",
    "    # Get some colors from seaborn:\n",
    "    colors = sns.color_palette(\"husl\", len(coefs[0]))\n",
    "    \n",
    "    # Get the figure and reset the size to be wider:\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,5)\n",
    "\n",
    "    # We have two axes this time on our figure. \n",
    "    # The fig.add_subplot adds axes to our figure. The number inside stands for:\n",
    "    #[figure_rows|figure_cols|position_of_current_axes]\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Give it the color cycler:\n",
    "    ax1.set_prop_cycle(cycler('color', colors))\n",
    "    \n",
    "    # Print a vertical line showing our current alpha threshold:\n",
    "    ax1.axvline(to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    \n",
    "    # Plot the lines of the alphas on x-axis and coefficients on y-axis\n",
    "    ax1.plot(alphas, coefs, lw=2)\n",
    "    \n",
    "    # set labels for axes:\n",
    "    ax1.set_xlabel('alpha', fontsize=20)\n",
    "    ax1.set_ylabel('coefficients', fontsize=20)\n",
    "    \n",
    "    # If this is for the ridge, set this to a log scale on the x-axis:\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Enforce the axis limits:\n",
    "    ax1.set_xlim([amin, amax])\n",
    "    ax1.set_ylim(1E-2,1E5)\n",
    "    \n",
    "    # Put a title on the axis\n",
    "    ax1.set_title(regtype+' coef paths\\n', fontsize=20)\n",
    "    \n",
    "    # Get the ymin and ymax for this axis to enforce it to be the same on the \n",
    "    # second chart:\n",
    "    ymin, ymax = ax1.get_ylim()\n",
    "\n",
    "    # Add our second axes for the barplot in position 2:\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylim(1E-2,1E5)\n",
    "    \n",
    "    # Position the bars according to their index from the feature names variable:\n",
    "    \n",
    "    try:\n",
    "        ax2.bar(range(len(coefs[-1])), coefs[-1], align='center', color=colors)\n",
    "    except ValueError:\n",
    "        pass\n",
    "#     ax2.set_xticks(range(1, len(feature_names)+1))\n",
    "    \n",
    "    # enforce limits and add titles, labels\n",
    "    ax2.set_ylim([ymin, ymax])\n",
    "    ax2.set_title(regtype+' predictor coefs\\n', fontsize=20)\n",
    "    ax2.set_xlabel('coefficients', fontsize=20)\n",
    "    ax2.set_ylabel('alpha', fontsize=20)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ipython widgets so we can make this plotting function interactive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and `interact` from ipywidgets lets me take some specified alphas that we have already calculated the coefficients for and plot them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_plot_runner(log_of_alpha=-3.0):\n",
    "    coef_plotter(r_alphas, r_coefs, 10**log_of_alpha, regtype='ridge')\n",
    "\n",
    "interact(ridge_plot_runner, log_of_alpha=(-3.0,10.0,0.33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-lasso'></a>\n",
    "\n",
    "## Visualizing the Lasso\n",
    "\n",
    "---\n",
    "\n",
    "Now we do the same thing as above but for the Lasso. You will be able to see how the coefficients change differently for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the ridge coefficient by alpha calculator\n",
    "def lasso_coefs(X, Y, alphas):\n",
    "    coefs = []\n",
    "    lasso_reg = Lasso()\n",
    "    for a in alphas:\n",
    "        lasso_reg.set_params(alpha=a)\n",
    "        lasso_reg.fit(X, Y)\n",
    "        coefs.append(lasso_reg.coef_)\n",
    "        \n",
    "    return coefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alphas for the Lasso tend to effect regularization linearly rather than by orders of magnitude like in the ridge. \n",
    "\n",
    "A linear series of alphas is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_alphas = np.logspace(-3,12,45)\n",
    "l_coefs = lasso_coefs(dataset_2, target_2, l_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same plotting function above, but now with the calculated coefficients of alpha for the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_plot_runner(log_of_alpha=-3):\n",
    "    coef_plotter(l_alphas, l_coefs, 10**log_of_alpha, regtype='lasso')\n",
    "\n",
    "interact(lasso_plot_runner, log_of_alpha=(-3.0,10.0,0.33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs = GridSearchCV(Lasso(), param_grid={'alpha' : np.logspace(-3,5,9)}, n_jobs=-1, cv=5)\n",
    "lasso_gs.fit(dataset_1, target_1)\n",
    "lasso_results = pd.DataFrame(gs.cv_results_)\n",
    "lasso_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_for_dataset(dataset, target, dataset_name, alphas):\n",
    "    lasso_gs = GridSearchCV(Lasso(), param_grid={'alpha': alphas}, n_jobs=-1, cv=5)\n",
    "    lasso_gs.fit(dataset, target)\n",
    "    lasso_results = pd.DataFrame(lasso_gs.cv_results_)\n",
    "    lasso_results.set_index('param_alpha', inplace=True)\n",
    "    ridge_gs = GridSearchCV(Ridge(), param_grid={'alpha' : alphas}, n_jobs=-1, cv=5)\n",
    "    ridge_gs.fit(dataset, target)\n",
    "    ridge_results = pd.DataFrame(ridge_gs.cv_results_)\n",
    "    ridge_results.set_index('param_alpha', inplace=True)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "    fig.suptitle(dataset_name)\n",
    "\n",
    "    lasso_results[['mean_train_score', 'mean_test_score']].plot(logx=True, ylim=(0.5,1.0), ax=ax[0])\n",
    "    ridge_results[['mean_train_score', 'mean_test_score']].plot(logx=True, ylim=(0.5,1.0), ax=ax[1])\n",
    "\n",
    "    lasso_max_test_score = round(lasso_results.mean_test_score.max(), 4)\n",
    "    ridge_max_test_score = round(ridge_results.mean_test_score.max(), 4)\n",
    "\n",
    "    ax[0].axhline(lasso_max_test_score, color='red', label='max')\n",
    "    mid_alphas = int(len(alphas)/2+1)\n",
    "    ax[0].text(alphas[mid_alphas], lasso_max_test_score + .01, str(lasso_max_test_score))\n",
    "    ax[0].set_title('Lasso')\n",
    "    ax[1].axhline(ridge_max_test_score, color='red', label='max')\n",
    "    ax[1].text(alphas[mid_alphas], ridge_max_test_score + .01, str(ridge_max_test_score))\n",
    "    ax[1].set_title('Ridge');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_dataset(dataset_1, target_1, 'Dataset 1', np.logspace(-3,5,9))\n",
    "plot_for_dataset(dataset_2, target_2, 'Dataset 2', np.logspace(-3,5,9))\n",
    "plot_for_dataset(dataset_3, target_3, 'Dataset 3', np.logspace(-3,5,9))\n",
    "plot_for_dataset(dataset_4, target_4, 'Dataset 4', np.logspace(-3,5,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_dataset(dataset_1, target_1, 'Dataset 1', np.logspace(1,3,5))\n",
    "plot_for_dataset(dataset_2, target_2, 'Dataset 2', np.logspace(1,3,5))\n",
    "plot_for_dataset(dataset_3, target_3, 'Dataset 3', np.logspace(1,3,5))\n",
    "plot_for_dataset(dataset_4, target_4, 'Dataset 4', np.logspace(1,3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs = GridSearchCV(Lasso(), param_grid={'alpha': np.logspace(1,3,5)}, n_jobs=-1, cv=5)\n",
    "lasso_gs.fit(dataset_2, target_2)\n",
    "lasso_results = pd.DataFrame(lasso_gs.cv_results_)\n",
    "lasso_results.set_index('param_alpha', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame(lasso_gs.best_estimator_.coef_, index=dataset_2.columns, columns=['value'])\n",
    "coefficients = coefficients[coefficients.value != 0 ]\n",
    "coefficients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients['abs'] = np.abs(coefficients.value)\n",
    "coefficients.sort_values('abs', ascending=False).head(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "03860d2d80204ca295d01e93e8e99474": {
     "views": [
      {
       "cell_index": 41
      }
     ]
    },
    "b535fb165fa343b297ba42fb4a55c6fa": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    },
    "f5d5ef714eee4c61b085a3bb6b96cd73": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "fdc5e91596ea49fa84bf4aed6d37b849": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
