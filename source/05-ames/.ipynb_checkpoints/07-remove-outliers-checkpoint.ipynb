{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "In the file `src/preprocessing.py`, we perform the preprocessing steps necessary for the next phase of this project. This file does the following:\n",
    "\n",
    "1. Load and merge data, using the `Id` feature as we did previously\n",
    "2. Assign correct data types, in particular, designating which features are categorical\n",
    "3. Handle missing values, using the work we did previously\n",
    "4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run src/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Feature Sets\n",
    "\n",
    "We are starting to do some fairly complicated feature engineering. It makes sense that we should spend some time thinking about the different data sets we are creating so that we can keep track of what we have.\n",
    "\n",
    "##### Standard Scaled Data Set\n",
    "\n",
    "One data set is the standard scaled data set. For this data set, there is no need to separate the encoded categorical features. These two dataframes comprise a complete data set\n",
    "\n",
    "- Log Transformed, Standard Scaled Numerical Features (`numeric_log_std_sc_df`)\n",
    "- Complete One-hot Encoded Categorical Features (`categorical_encoded_df`)\n",
    "\n",
    "##### Gelman Scaled Data Set\n",
    "\n",
    "The other data set is the Gelman scaled data set. For this data set, we have separated the encoded categorical features based on a threshold for variance. These three dataframes comprise a complete data set\n",
    "\n",
    "- Log Transformed, Gelman Scaled Numerical Features (`numeric_log_gel_sc_df`)\n",
    "- One-hot Encoded Categorical Features with Significant Variance, Centered (`categorical_encoded_features_significant_variance_centered`)\n",
    "- One-hot Encoded Categorical Features with Insignificant Variance (`categorical_encoded_features_insignificant_variance`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work with the numeric features to identify outliers. As before, we will use the Tukey Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_outliers(dataframe, col, param=1.5):\n",
    "    Q1 = np.percentile(dataframe[col], 25)\n",
    "    Q3 = np.percentile(dataframe[col], 75)\n",
    "    tukey_window = param*(Q3-Q1)\n",
    "    less_than_Q1 = dataframe[col] < Q1 - tukey_window\n",
    "    greater_than_Q3 = dataframe[col] > Q3 + tukey_window\n",
    "    tukey_mask = (less_than_Q1 | greater_than_Q3)\n",
    "    return dataframe[tukey_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column.             Standard      Gelman \")\n",
    "print(\"--------------------------------------------\")\n",
    "for col in numeric_log_std_sc_df.columns:\n",
    "    print(\"{:20} {:12} {}\".format(col, \n",
    "                               str(display_outliers(numeric_log_std_sc_df, col).shape),\n",
    "                               str(display_outliers(numeric_log_gel_sc_df, col).shape)\n",
    "                              ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both scaling techniques return the same number of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Multiple Outliers\n",
    "\n",
    "As before, we will count row that are outlier for more than one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_outliers(dataframe, count=2):\n",
    "    raw_outliers = []\n",
    "    for col in dataframe:\n",
    "        outlier_df = feature_outliers(dataframe, col)\n",
    "        raw_outliers += list(outlier_df.index)\n",
    "\n",
    "    outlier_count = Counter(raw_outliers)\n",
    "    outliers = [k for k,v in outlier_count.items() if v >= count]\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiple_outliers(numeric_log_std_sc_df)), len(multiple_outliers(numeric_log_gel_sc_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the two scaling techniques return the same number of multiple outliers. Unfortunately, this number of outliers represents an unacceptable loss of data, approximately 20% of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiple_outliers(numeric_log_std_sc_df))/numeric_log_std_sc_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the multiple feature count higher and reassess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(multiple_outliers(numeric_log_std_sc_df, count=4)), len(multiple_outliers(numeric_log_gel_sc_df, count=4)))\n",
    "print(len(multiple_outliers(numeric_log_std_sc_df, count=4))/numeric_log_std_sc_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiple_outliers(numeric_log_std_sc_df, count=5)), len(multiple_outliers(numeric_log_gel_sc_df, count=5))\n",
    "print(len(multiple_outliers(numeric_log_std_sc_df, count=5))/numeric_log_std_sc_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances that are an outlier in four or more features amount to 1.3% of the data. Instances that are an outlier in five or more features amount to 0.5% of the data.  Both of these represent acceptable losses. Here we will use Instances that are outlier in five or more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_log_std_sc_out_rem_df = numeric_log_std_sc_df.drop(multiple_outliers(numeric_log_std_sc_df, 5))\n",
    "numeric_log_gel_sc_out_rem_df = numeric_log_gel_sc_df.drop(multiple_outliers(numeric_log_gel_sc_df, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
